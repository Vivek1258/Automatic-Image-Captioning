{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "delayed-indianapolis",
   "metadata": {},
   "source": [
    "# Model Developement, Treaning and Evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-masters",
   "metadata": {},
   "source": [
    "## Setting up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "prerequisite-worthy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# importing the necossery liberay \n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from numpy import argmax\n",
    "from pickle import load\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "breeding-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an object that represents the SageMaker session that we are currently operating in. This\n",
    "# object contains some useful information that we will need to access later such as our region.\n",
    "Sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# This is an object that represents the IAM role that we are currently assigned. When we construct\n",
    "# and launch the training job later we will need to tell it what IAM role it should have. Since our\n",
    "# use case is relatively simple we will simply assign the training job the role we currently have.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "satellite-demographic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/SageMaker/Modeling '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "studied-afghanistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"/home/ec2-user/SageMaker/Data Preprocessing/Dataset/train\"\n",
    "test_dir  = \"/home/ec2-user/SageMaker/Data Preprocessing/Dataset/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-multimedia",
   "metadata": {},
   "source": [
    "## Uploading the data files to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-major",
   "metadata": {},
   "source": [
    "### Preprocessed data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "internal-argentina",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the preprossed data to this sagemaker instance's s3 \n",
    "\n",
    "s3_prefix = 'imagecaptioning_vivek' \n",
    "\n",
    "traindata_s3_prefix = '{}/data/train'.format(s3_prefix)\n",
    "testdata_s3_prefix = '{}/data/test'.format(s3_prefix)\n",
    "\n",
    "train_s3 = Sagemaker_session.upload_data(path= train_dir, key_prefix=traindata_s3_prefix)\n",
    "test_s3 = Sagemaker_session.upload_data(path=  test_dir, key_prefix=testdata_s3_prefix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-mills",
   "metadata": {},
   "source": [
    "### Creating Inputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dimensional-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\n",
    "            'train':train_s3,\n",
    "            'test': test_s3,\n",
    "         }   # locations of the data in the S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "indie-identifier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 's3://sagemaker-us-east-1-478270364551/imagecaptioning_vivek/data/train',\n",
       " 'test': 's3://sagemaker-us-east-1-478270364551/imagecaptioning_vivek/data/test'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-reservation",
   "metadata": {},
   "source": [
    "## Model Developement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dedicated-sport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[37m## Get the model \u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mmodel\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_model \r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_training_data\u001b[39;49;00m(train_dir):\r\n",
      "    \u001b[33m\"\"\" loading training data from S3 \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    X1train = np.load(os.path.join(train_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mX1train.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33marr_0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    X2train = np.load(os.path.join(train_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mX2train.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33marr_0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    ytrain = np.load(os.path.join(train_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mytrain.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33marr_0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m X1train,  X2train , ytrain\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_load_testing_data\u001b[39;49;00m(test_dir):\r\n",
      "    \u001b[33m\"\"\" loading testing data from S3 \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    X1test = np.load(os.path.join(test_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mX1test.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33marr_0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    X2test = np.load(os.path.join(test_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mX2test.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33marr_0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    ytest = np.load(os.path.join(test_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mytest.npz\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))[\u001b[33m'\u001b[39;49;00m\u001b[33marr_0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m X1test,  X2test , ytest\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_parse_args\u001b[39;49;00m():\r\n",
      "    \r\n",
      "    \u001b[33m\"\"\" Parsing the arguments \"\"\"\u001b[39;49;00m\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\r\n",
      "    \u001b[37m# model_dir is always passed in from SageMaker. By default, this is an S3 path under the default bucket.\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--sm-model-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_known_args()\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "\r\n",
      "    \u001b[37m# parsing the arguments\u001b[39;49;00m\r\n",
      "    args, unknown = _parse_args()\r\n",
      "\r\n",
      "    \u001b[37m# loading the train and test data\u001b[39;49;00m\r\n",
      "    X1train,  X2train , ytrain = _load_training_data(args.train)\r\n",
      "    X1test,  X2test , ytest = _load_testing_data(args.test)\r\n",
      "    \r\n",
      "    \u001b[37m# get model  \u001b[39;49;00m\r\n",
      "\r\n",
      "    model = get_model()\r\n",
      "\r\n",
      "    \u001b[37m#compile\u001b[39;49;00m\r\n",
      "\r\n",
      "    opt = tf.keras.optimizers.Adam(lr = \u001b[34m0.001\u001b[39;49;00m)\r\n",
      "    BATCH__SIZE = \u001b[34m128\u001b[39;49;00m\r\n",
      "\r\n",
      "    model.compile(optimizer=opt,\r\n",
      "              loss=\u001b[33m'\u001b[39;49;00m\u001b[33mcategorical_crossentropy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "              )\r\n",
      "\r\n",
      "    \u001b[37m# train on 10 epoches\u001b[39;49;00m\r\n",
      "    model.fit(\r\n",
      "          [ X1train,  X2train ], \r\n",
      "          ytrain, \r\n",
      "          batch_size = BATCH__SIZE ,\r\n",
      "          epochs=\u001b[34m3\u001b[39;49;00m, \r\n",
      "          verbose=\u001b[34m2\u001b[39;49;00m,\r\n",
      "          validation_data=( [ X1test,  X2test ] , ytest)\r\n",
      "          )\r\n",
      "          \r\n",
      "    \u001b[36mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mSaving The model ....\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m )\r\n",
      "    \r\n",
      "    model.save(os.path.join(args.sm_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33m0001\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m), \u001b[33m'\u001b[39;49;00m\u001b[33mmy_model_ic.h5\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'Train/train.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "coastal-safety",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# importing neccossory liberies\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mget_model\u001b[39;49;00m(input_im_feature_shape = \u001b[34m1000\u001b[39;49;00m  , vocab_size = \u001b[34m7579\u001b[39;49;00m , max_length = \u001b[34m34\u001b[39;49;00m ):\r\n",
      "    \r\n",
      "    \r\n",
      "    \u001b[33m\"\"\"\u001b[39;49;00m\r\n",
      "\u001b[33m    creates an encoder decocer  arcitechure\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Parameters:\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    input_im_feature_shape \u001b[39;49;00m\r\n",
      "\u001b[33m    ==> The shape of the feature vector \u001b[39;49;00m\r\n",
      "\u001b[33m    of the images extrated using Transfer learniong model \u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    vocab_size ==> size of the vocabolary ( determined during data preprocessing )\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    max_length ==> maximum length of input sequence \u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m    keras.models.Model object \u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m# feature extractor model\u001b[39;49;00m\r\n",
      "\r\n",
      "    inputs1 = tf.keras.layers.Input(shape=(input_im_feature_shape,))\r\n",
      "    \r\n",
      "    fe1 = tf.keras.layers.Dense(\u001b[34m256\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(inputs1)\r\n",
      "    fe2 = tf.keras.layers.Dropout(\u001b[34m0.3\u001b[39;49;00m)(fe1)\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[37m# sequence model\u001b[39;49;00m\r\n",
      "    inputs2 = tf.keras.layers.Input(shape=(max_length,))\r\n",
      "    \r\n",
      "    se1 = tf.keras.layers.Embedding(vocab_size, \u001b[34m256\u001b[39;49;00m, mask_zero=\u001b[34mTrue\u001b[39;49;00m)(inputs2)\r\n",
      "    se2 = tf.keras.layers.Dropout(\u001b[34m0.3\u001b[39;49;00m)(se1)\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[37m#merge layer\u001b[39;49;00m\r\n",
      "    merge = tf.keras.layers.Add()([fe2, se2]) \r\n",
      "\r\n",
      "    \u001b[37m#language model\u001b[39;49;00m\r\n",
      "\r\n",
      "    decoder = tf.keras.layers.LSTM(\u001b[34m256\u001b[39;49;00m)(merge)\r\n",
      "     \r\n",
      "\r\n",
      "    \u001b[37m#fully connected\u001b[39;49;00m\r\n",
      "\r\n",
      "    decoder__ = tf.keras.layers.Dense(\u001b[34m1024\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(decoder)\r\n",
      "    bn = tf.keras.layers.BatchNormalization()(decoder__)\r\n",
      "    do = tf.keras.layers.Dropout(\u001b[34m0.1\u001b[39;49;00m)(bn)\r\n",
      "\r\n",
      "\r\n",
      "    outputs = tf.keras.layers.Dense(vocab_size, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msoftmax\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(do)\r\n",
      "\r\n",
      "    \u001b[37m# tie it together [image, seq] [word]\u001b[39;49;00m\r\n",
      "\r\n",
      "    model = tf.keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)\r\n",
      "\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize 'Train/model.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "intense-corporation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Train.train import get_model\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "continent-plenty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1000)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          256256      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 34, 256)      1940224     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 34, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 34, 256)      0           dropout[0][0]                    \n",
      "                                                                 dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          525312      add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         263168      lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 1024)         4096        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7579)         7768475     dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,757,531\n",
      "Trainable params: 10,755,483\n",
      "Non-trainable params: 2,048\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-paraguay",
   "metadata": {},
   "source": [
    "## Model Treaning  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "equal-terrorism",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributions has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "\n",
    "estimator = TensorFlow(entry_point='train.py',\n",
    "                     source_dir=\"Train\",\n",
    "                     role=role,\n",
    "                     train_instance_count=1,\n",
    "                     train_instance_type='ml.p2.xlarge',\n",
    "                     framework_version='2.1.0',\n",
    "                     py_version='py3',\n",
    "                     distributions={'parameter_server': {'enabled': True}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cardiac-biotechnology",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-13 10:49:22 Starting - Starting the training job...\n",
      "2021-03-13 10:49:46 Starting - Launching requested ML instancesProfilerReport-1615632562: InProgress\n",
      "......\n",
      "2021-03-13 10:50:50 Starting - Preparing the instances for training.........\n",
      "2021-03-13 10:52:09 Downloading - Downloading input data...\n",
      "2021-03-13 10:52:48 Training - Downloading the training image........\u001b[34m2021-03-13 10:54:04,133 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2021-03-13 10:54:04,628 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_parameter_server_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_dir\": \"s3://sagemaker-us-east-1-478270364551/tensorflow-training-2021-03-13-10-49-22-279/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tensorflow-training-2021-03-13-10-49-22-279\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-478270364551/tensorflow-training-2021-03-13-10-49-22-279/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"model_dir\":\"s3://sagemaker-us-east-1-478270364551/tensorflow-training-2021-03-13-10-49-22-279/model\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_parameter_server_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-478270364551/tensorflow-training-2021-03-13-10-49-22-279/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_parameter_server_enabled\":true},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"model_dir\":\"s3://sagemaker-us-east-1-478270364551/tensorflow-training-2021-03-13-10-49-22-279/model\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2021-03-13-10-49-22-279\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-478270364551/tensorflow-training-2021-03-13-10-49-22-279/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--model_dir\",\"s3://sagemaker-us-east-1-478270364551/tensorflow-training-2021-03-13-10-49-22-279/model\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-us-east-1-478270364551/tensorflow-training-2021-03-13-10-49-22-279/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 train.py --model_dir s3://sagemaker-us-east-1-478270364551/tensorflow-training-2021-03-13-10-49-22-279/model\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2021-03-13 10:54:09 Training - Training image download completed. Training in progress.\u001b[34m2021-03-13 10:54:53.767600: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 9288943664 exceeds 10% of system memory.\u001b[0m\n",
      "\u001b[34mTrain on 306404 samples, validate on 50903 samples\u001b[0m\n",
      "\u001b[34mEpoch 1/3\u001b[0m\n",
      "\u001b[34m306404/306404 - 183s - loss: 4.0741 - val_loss: 3.6243\u001b[0m\n",
      "\u001b[34mEpoch 2/3\u001b[0m\n",
      "\u001b[34m306404/306404 - 184s - loss: 3.3640 - val_loss: 3.5017\u001b[0m\n",
      "\u001b[34mEpoch 3/3\u001b[0m\n",
      "\u001b[34m306404/306404 - 182s - loss: 3.0324 - val_loss: 3.4882\u001b[0m\n",
      "\u001b[34mSaving The model ....\u001b[0m\n",
      "\u001b[34m2021-03-13 11:04:16.095506: W tensorflow/python/util/util.cc:319] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mIf using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mIf using Keras pass *_constraint arguments to layers.\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/0001/assets\u001b[0m\n",
      "\u001b[34mINFO:tensorflow:Assets written to: /opt/ml/model/0001/assets\u001b[0m\n",
      "\n",
      "2021-03-13 11:04:22 Uploading - Uploading generated training model\u001b[34m2021-03-13 11:04:21,019 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-03-13 11:04:56 Completed - Training job completed\n",
      "Training seconds: 756\n",
      "Billable seconds: 756\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "alien-penny",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data=estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "divided-triumph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-478270364551/tensorflow-training-2021-03-13-10-49-22-279/output/model.tar.gz'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-haiti",
   "metadata": {},
   "source": [
    "##  Testing \n",
    "\n",
    "### We first deploy the model and use the model for the testing \n",
    "\n",
    "NOTE : Its different from what we do in the production level deployment (there  we create a sagemaker.tensorflow.TensorFlowModel object )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "instant-politics",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "update_endpoint is a no-op in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "# deploying th eendpoint \n",
    "\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "metallic-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = load(open('/home/ec2-user/SageMaker/Data Preprocessing/Dataset/tokenizer.pkl' ,'rb'))\n",
    "\n",
    "test_features = load(open('/home/ec2-user/SageMaker/Data Preprocessing/Dataset/test_features' ,'rb'))\n",
    "\n",
    "test_descriptions = load(open('/home/ec2-user/SageMaker/Data Preprocessing/Dataset/test_descriptions' ,'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "heated-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_desc( tokenizer, feature , max_length = 34 ):\n",
    "    \n",
    "    \"\"\" funcion to generate a description for an image\"\"\"\n",
    "    \n",
    "    in_text = 'startseq' # to seed the generation process\n",
    "    \n",
    "     \n",
    "    for i in range(max_length):\n",
    "        \n",
    "        # integer encode and pad input sequence \n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "    \n",
    "        # predict next word\n",
    "        inputs = {\n",
    "                  \"input_1\": feature.tolist() , \n",
    "                    \"input_2\": sequence[0].tolist()\n",
    "                    }\n",
    "\n",
    "        result = predictor.predict(inputs)\n",
    "        \n",
    "        yhat = np.argmax(result['predictions'])\n",
    " \n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        \n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        \n",
    "        in_text += ' ' + word # append the input for generating the next word\n",
    "        \n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "            \n",
    "            \n",
    "    return in_text\n",
    "\n",
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    "    \n",
    "    \"\"\" function to map an integer to a word\"\"\"\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "proprietary-yahoo",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "actual, predicted = list(), list()\n",
    "\n",
    "\n",
    "def evaluate_model(descriptions, features, tokenizer, max_length = 34 ):\n",
    "    \n",
    "    \"\"\"Function to  evaluate the skill of the model\"\"\"\n",
    "    \n",
    "    # step over the whole set\n",
    "    for key, desc_list in descriptions.items():\n",
    "        \n",
    "        #print(features[key][0].shape)\n",
    "        \n",
    "        # generate description\n",
    "        yhat = generate_desc(tokenizer, features[key][0])\n",
    "        \n",
    "        # store actual and predicted\n",
    "        references = [d.split() for d in desc_list]\n",
    "        \n",
    "        actual.append(references)\n",
    "        \n",
    "        predicted.append(yhat.split())\n",
    "        \n",
    "    # calculate BLEU score\n",
    "    print('BLEU: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "valid-mills",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 0.521894\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(test_descriptions, test_features, tokenizer, max_length = 34 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-plumbing",
   "metadata": {},
   "source": [
    "## Shutting Down The endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "pointed-apollo",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Preprocessing__IC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRoGOrmapucp"
      },
      "source": [
        "# Data Preprocessing \r\n",
        "\r\n",
        "1. Download the data.\r\n",
        "2. Process / Prepare the data.\r\n",
        "\r\n",
        "    *   Image Data Preprocessing \r\n",
        "    *   Text Data Preprocessing \r\n",
        "\r\n",
        "3. Save the processed dataset \r\n",
        "\r\n",
        "  *   Create treaning and testing datasets \r\n",
        "  *   Save treaning and testing data as npz( compressed numpy files )\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_TsPKHtpyP0"
      },
      "source": [
        "## Step 1: Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfcJJ_qupckf"
      },
      "source": [
        "# Imports \r\n",
        "\r\n",
        "## For uploading dataset \r\n",
        "\r\n",
        "import os \r\n",
        "from google.colab import drive\r\n",
        "from google.colab import files\r\n",
        "\r\n",
        "## for CNN_Encoder (Feature extrator model)\r\n",
        "\r\n",
        "from pickle import dump\r\n",
        "from os import listdir\r\n",
        "import string\r\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\r\n",
        "from tensorflow.keras.preprocessing.image import load_img\r\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\r\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\r\n",
        "from tensorflow.keras.models import Model\r\n",
        "\r\n",
        "\r\n",
        "from numpy import array\r\n",
        "from pickle import load\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "## "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 179
        },
        "id": "9zTWtjVvp22K",
        "outputId": "7a91b693-ec1f-4a3d-e876-ff2a2811e176"
      },
      "source": [
        "###################### Mounting the drive ##############################\r\n",
        "\r\n",
        "drive.mount('/content/gdrive')\r\n",
        "\r\n",
        "####################### Uploading the kaggle API key ###################\r\n",
        "\r\n",
        "files.upload() #this will prompt you to update the json\r\n",
        "\r\n",
        "!pip install -q kaggle\r\n",
        "!mkdir -p ~/.kaggle\r\n",
        "!cp kaggle.json ~/.kaggle/\r\n",
        "!ls ~/.kaggle\r\n",
        "!chmod 600 /root/.kaggle/kaggle.json  # set permission\r\n",
        "\r\n",
        "####################### Downloading the dataset ########################\r\n",
        "\r\n",
        "!kaggle datasets download -d shadabhussain/flickr8k\r\n",
        "\r\n",
        "os.chdir('/content')  #change dir\r\n",
        "!mkdir Flicker8k_Dataset  #create a directory \r\n",
        "!unzip -q flickr8k.zip -d Flicker8k_Dataset\r\n",
        "\r\n",
        "os.listdir('/content/Flicker8k_Dataset/')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cb9cd4f8-d455-4af8-8709-733012ecea1a\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cb9cd4f8-d455-4af8-8709-733012ecea1a\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "kaggle.json\n",
            "Downloading flickr8k.zip to /content\n",
            " 99% 2.10G/2.13G [00:21<00:00, 84.3MB/s]\n",
            "100% 2.13G/2.13G [00:21<00:00, 107MB/s] \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model_weights.h5', 'train_encoded_images.p', 'flickr_data', 'Flickr_Data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf1W0rxqqCrI"
      },
      "source": [
        "## Step 2: Preparing and Processing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rGQUI2XqGFt"
      },
      "source": [
        "### Image Preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkzMluK5p6Yg"
      },
      "source": [
        "\r\n",
        "def process_img(filename):\r\n",
        "      \"\"\"\r\n",
        "\r\n",
        "      Function to load and process the image file,\r\n",
        "\r\n",
        "      Image needs to be modified and preprocessed before feeding it to the\r\n",
        "      pretrained model for VGG16 the input image dimentions should be (224, 224) \r\n",
        "      and needs to be processed accordingly by using preprocess_input from\r\n",
        "      tensorflow.keras.applications.vgg16 \r\n",
        "\r\n",
        "      INPUT : Image File path \r\n",
        "      Output : processed Image \r\n",
        "\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "\r\n",
        "      image = load_img(filename, target_size=(224, 224))\r\n",
        "      image = img_to_array(image)  # convert the image pixels to a numpy array\r\n",
        "      image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))  # reshape data for the model\r\n",
        "      image = preprocess_input(image)  # prepare the image for the VGG model\r\n",
        "\r\n",
        "      return image \r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLmhPuEfqKkW"
      },
      "source": [
        "\r\n",
        "def extract_features(directory):\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "      Function to extract features from an image using Pretrained Model\r\n",
        "      Here we are using VGG16 model \r\n",
        "\r\n",
        "      Input : Path to image directory \r\n",
        "      Output : A dict =  { key -->  Image_ID \r\n",
        "                           value -->  Extracted Features  \r\n",
        "                           }\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "\r\n",
        "      # Prepare the model \r\n",
        "      model = VGG16()\r\n",
        "      model.layers.pop()\r\n",
        "      model = Model(inputs=model.inputs, \r\n",
        "                    outputs=model.layers[-1].output)\r\n",
        "      print(model.summary())\r\n",
        "\r\n",
        "      # extract features from each photo\r\n",
        "      features = dict()\r\n",
        "\r\n",
        "      for name in listdir(directory):\r\n",
        "            image_id = name.split('.')[0]\r\n",
        "            filename = directory + '/' + name\r\n",
        "            image = process_img(filename)\r\n",
        "            feature = model.predict(image, verbose=0)\r\n",
        "            features[image_id] = feature # store feature\r\n",
        "            break\r\n",
        "        \r\n",
        "      return features\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRsFL3meqMlo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4271cb21-6987-4941-b2c4-035c2f53f5fc"
      },
      "source": [
        "########    extract features from all images\r\n",
        "\r\n",
        "directory = '/content/Flicker8k_Dataset/Flickr_Data/Flickr_Data/Images'\r\n",
        "features = extract_features(directory)\r\n",
        "#print('Extracted Features: %d' % len(features))\r\n",
        "\r\n",
        "# save to file\r\n",
        "dump(features, open('features.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 7s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 4096)              102764544 \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 4096)              16781312  \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 1000)              4097000   \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqW3szjTqW3d"
      },
      "source": [
        "### Text Preprocessing\r\n",
        "\r\n",
        "####  extract descriptions for images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E96NFLiPqRmg"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def load_and_clean_descriptions(filename):\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "      Function to Load and Clean the descriptions of the images \r\n",
        "\r\n",
        "      Input : Path to the raw Flickr8k.token.txt file \r\n",
        "\r\n",
        "      Output : A Dictonary = {\r\n",
        "                              key -->  Image_ID \r\n",
        "                              value --> A list,  containg descriptions of image \r\n",
        "                              }\r\n",
        "      \"\"\"\r\n",
        "\r\n",
        "      # Load the file \r\n",
        "      file = open(filename, 'r')\r\n",
        "      doc = file.read()\r\n",
        "      file.close()  \r\n",
        "\r\n",
        "      mapping = dict()\r\n",
        "\r\n",
        "      table = str.maketrans('', '', string.punctuation) # To remove punctuation from each token\r\n",
        "\r\n",
        "\r\n",
        "      for line in doc.split('\\n'):\r\n",
        "\r\n",
        "            tokens = line.split()\r\n",
        "            if len(tokens)>2 :          \r\n",
        "                image_id, desc = tokens[0], tokens[1:] #first token is the image id, the rest is the description\r\n",
        "                image_id = image_id.split('.')[0] # sample Image path :: 1000268201_693b08cb0e.jpg\r\n",
        "\r\n",
        "                ## Clean the Description\r\n",
        "\r\n",
        "                desc = [word.lower() for word in desc]\r\n",
        "                desc = [w.translate(table) for w in desc]\r\n",
        "                desc = [word for word in desc if len(word)>1] # removing short words \r\n",
        "                image_desc = ' '.join(desc) # making it a string again \r\n",
        "\r\n",
        "                 \r\n",
        "\r\n",
        "                if image_id not in mapping:\r\n",
        "                      mapping[image_id] = list()\r\n",
        "\r\n",
        "                mapping[image_id].append(image_desc)\r\n",
        "\r\n",
        "      return mapping\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3ZhJnptqasb"
      },
      "source": [
        "\r\n",
        "def to_vocabulary(descriptions):\r\n",
        "      \"\"\"\r\n",
        "      Function to create a vocabulary of words from the avalable text data \r\n",
        "      \r\n",
        "      Input :  : A Dictonary = {\r\n",
        "                                  key -->  Image_ID \r\n",
        "                                  value --> A list,  containg descriptions of image \r\n",
        "                                  }\r\n",
        "      \r\n",
        "      Output : A Set containg all unique words in the descriptions of all images \r\n",
        "      \r\n",
        "      \"\"\"\r\n",
        "      # build a list of all description strings\r\n",
        "      all_desc = set()\r\n",
        "      for key in descriptions.keys():\r\n",
        "          [all_desc.update(d.split()) for d in descriptions[key]]\r\n",
        "\r\n",
        "      return all_desc\r\n",
        " \r\n",
        "\r\n",
        "\r\n",
        "# \r\n",
        "def save_descriptions(descriptions, filename):\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "      Function to save descriptions to file, one per line \r\n",
        "      \r\n",
        "      Input : 1. Dictonary = {\r\n",
        "                                  key -->  Image_ID \r\n",
        "                                  value --> A list,  containg descriptions of image \r\n",
        "                                  }\r\n",
        "              2. Path of the file to be saved \r\n",
        "      \r\n",
        "      Output : None \r\n",
        "      \r\n",
        "      \"\"\"\r\n",
        "      lines = list()\r\n",
        "      for key, desc_list in descriptions.items():\r\n",
        "            for desc in desc_list:\r\n",
        "                  lines.append(key + ' ' + desc)\r\n",
        "\r\n",
        "      data = '\\n'.join(lines)\r\n",
        "      file = open(filename, 'w')\r\n",
        "      file.write(data)\r\n",
        "      file.close()\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4eSVyl6qcug",
        "outputId": "ac8f3592-c6cc-4514-bcdf-2abc0e3be494"
      },
      "source": [
        "\r\n",
        "filename = '/content/Flicker8k_Dataset/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr8k.token.txt'\r\n",
        " \r\n",
        "# parse descriptions\r\n",
        "descriptions = load_and_clean_descriptions(filename)\r\n",
        "print('Loaded: %d ' % len(descriptions))\r\n",
        "\r\n",
        "\r\n",
        "# summarize vocabulary\r\n",
        "vocabulary = to_vocabulary(descriptions)\r\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\r\n",
        "\r\n",
        "\r\n",
        "# save to file\r\n",
        "save_descriptions(descriptions, 'descriptions.txt')\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded: 8092 \n",
            "Vocabulary Size: 8808\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5vtB46vvtgH"
      },
      "source": [
        "!cp \"/content/descriptions.txt\" \"/content/gdrive/MyDrive/Imagecap\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clbrp92QGyRb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P04tD4f4G_hR"
      },
      "source": [
        "## Step 3 : Saving the processed dataset \r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvYdVtQuHiUw"
      },
      "source": [
        "### A. Create treaning and testing datasets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txzoHZ2EqfWa"
      },
      "source": [
        "\r\n",
        "def load_set(filename):\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "      function to get a total image IDs \r\n",
        "\r\n",
        "      Input : Path to text file containing Image IDs\r\n",
        "\r\n",
        "      Output: A set of all image IDs given in the file \r\n",
        "\r\n",
        "      \"\"\"\r\n",
        " \r\n",
        "      file = open(filename, 'r')\r\n",
        "      doc = file.read()\r\n",
        "      file.close()\r\n",
        "    \r\n",
        "      dataset = list()\r\n",
        "      for line in doc.split('\\n'): # process line by line\r\n",
        "        \r\n",
        "          if len(line) < 1:# skip empty lines\r\n",
        "              continue\r\n",
        "          identifier = line.split('.')[0] # get image ID\r\n",
        "          dataset.append(identifier)\r\n",
        "\r\n",
        "      return set(dataset)\r\n",
        "\r\n",
        "\r\n",
        " \r\n",
        " \r\n",
        "def load_clean_descriptions(filename, dataset):\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "      Function to load and prepare the descriptions to feed to RNN \r\n",
        "      i.e adding start and end tag to each sentence \r\n",
        "\r\n",
        "      Inputs : 1. Path to file storing the preprocessed descriptions \r\n",
        "               2. A Set of image IDs whose description is to be loaded \r\n",
        "      \r\n",
        "      Output : A Dictonary = {\r\n",
        "                              key -->  Image_ID \r\n",
        "                              value --> A list,  containg descriptions of image \r\n",
        "                              }\r\n",
        "      \"\"\"\r\n",
        " \r\n",
        "      file = open(filename, 'r')\r\n",
        "      doc = file.read()\r\n",
        "      file.close()\r\n",
        "    \r\n",
        "      descriptions = dict()\r\n",
        "      for line in doc.split('\\n'):\r\n",
        "\r\n",
        "          tokens = line.split() # split line by white space\r\n",
        "          image_id, image_desc = tokens[0], tokens[1:] # get id and description  \r\n",
        "\r\n",
        "          if image_id in dataset: # This is done to skip images not in the dataset         \r\n",
        "              if image_id not in descriptions: # create list\r\n",
        "                  descriptions[image_id] = list()       \r\n",
        "              desc = 'startseq ' + ' '.join(image_desc) + ' endseq' # wrap description in tokens\r\n",
        "              descriptions[image_id].append(desc)\r\n",
        "      \r\n",
        "      return descriptions\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8oOaWwS2pz3"
      },
      "source": [
        "\r\n",
        "\r\n",
        "def load_img_features(filename, dataset):\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "      Function to  load image features of the images in the given set of image IDs \r\n",
        "      \r\n",
        "      \"\"\"\r\n",
        "      \r\n",
        "      all_features = load(open(filename, 'rb'))# load all features\r\n",
        "      features = {k: all_features[k] for k in dataset}# filter features\r\n",
        "\r\n",
        "      return features\r\n",
        " \r\n",
        "\r\n",
        "def to_lines(descriptions):\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "      Function to covert a dictionary of clean descriptions to a list of descriptions\r\n",
        "      \r\n",
        "      \"\"\"\r\n",
        "      all_desc = list()\r\n",
        "      for key in descriptions.keys():\r\n",
        "          [all_desc.append(d) for d in descriptions[key]]\r\n",
        "\r\n",
        "      return all_desc\r\n",
        " \r\n",
        "# \r\n",
        "def create_tokenizer(descriptions):\r\n",
        "      \"\"\"\r\n",
        "\r\n",
        "      function to create, fit  a tokenizer given caption descriptions\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "      lines = to_lines(descriptions)\r\n",
        "      tokenizer = Tokenizer()\r\n",
        "      tokenizer.fit_on_texts(lines)\r\n",
        "\r\n",
        "      return tokenizer\r\n",
        " \r\n",
        "\r\n",
        "def max_length(descriptions):\r\n",
        "      \"\"\"\r\n",
        "      \r\n",
        "      function to calculate the length of the longest description  \r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "      lines = to_lines(descriptions)\r\n",
        "      \r\n",
        "      return max(len(d.split()) for d in lines)\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6yl6Tj6v79C"
      },
      "source": [
        "\r\n",
        "def create_sequences(tokenizer, max_length, descriptions, img_features ):\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "      \r\n",
        "      Function to create sequences of images, input sequences and output words for an image\r\n",
        "\r\n",
        "      Input :  1. tokenizer --> To convert sentences into a list of words \r\n",
        "               2. max_length --> To pad the input sequence till max_length \r\n",
        "               3. descriptions --> To create input sequence to output word pairs \r\n",
        "               4. img_features --> To create image input \r\n",
        "\r\n",
        "\r\n",
        "      \"\"\"\r\n",
        "      X1, X2, y = list(), list(), list()\r\n",
        "\r\n",
        "      for key, desc_list in descriptions.items(): # walk through each image identifier\r\n",
        "\r\n",
        "          for desc in desc_list: # walk through each description for the image\r\n",
        "\r\n",
        "                seq = tokenizer.texts_to_sequences([desc])[0] # encode the sequence\r\n",
        "                for i in range(1, len(seq)): # split one sequence into multiple X,y pairs\r\n",
        "\r\n",
        "                    in_seq, out_seq = seq[:i], seq[i] # split into input and output pair                  \r\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0] # pad input sequence                 \r\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0] # encode output sequence               \r\n",
        "\r\n",
        "\r\n",
        "                    X1.append(img_features[key][0])\r\n",
        "                    X2.append(in_seq)\r\n",
        "                    y.append(out_seq)\r\n",
        "\r\n",
        "      return array(X1), array(X2), array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyEGLalME-jh"
      },
      "source": [
        "####Training Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wVjhAd0FdVl"
      },
      "source": [
        "descriptions_path = '/content/gdrive/My Drive/Imagecap/descriptions.txt'\r\n",
        "featurs_path = '/content/gdrive/My Drive/Imagecap/features.pkl'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A457wbOXv_BX",
        "outputId": "b81736bb-f2f0-41ec-9d2d-3394c339f514"
      },
      "source": [
        "# train dataset\r\n",
        "\r\n",
        "Train_img_IDs = '/content/Flicker8k_Dataset/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.trainImages.txt'\r\n",
        "\r\n",
        "# creating a set of Treaning image IDs \r\n",
        "train = load_set(Train_img_IDs) \r\n",
        "print('Dataset: %d' % len(train))\r\n",
        "\r\n",
        "\r\n",
        "# Create Train descriptions\r\n",
        "train_descriptions = load_clean_descriptions(descriptions_path, train) \r\n",
        "print('Descriptions: train=%d' % len(train_descriptions))\r\n",
        " \r\n",
        "\r\n",
        "# Create Train features\r\n",
        "train_features = load_img_features(featurs_path, train)\r\n",
        "print('Photos: train=%d' % len(train_features))\r\n",
        "\r\n",
        "\r\n",
        "# prepare tokenizer\r\n",
        "tokenizer = create_tokenizer(train_descriptions)\r\n",
        "dump(tokenizer, open('/content/gdrive/My Drive/Imagecap/tokenizer.pkl', 'wb'))\r\n",
        "vocab_size = len(tokenizer.word_index) + 1\r\n",
        "print('Vocabulary Size: %d' % vocab_size)\r\n",
        "\r\n",
        "\r\n",
        "# determine the maximum sequence length\r\n",
        "max_length = max_length(train_descriptions)\r\n",
        "print('Description Length: %d' % max_length)\r\n",
        "\r\n",
        "\r\n",
        "# prepare sequences\r\n",
        "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 6000\n",
            "Descriptions: train=6000\n",
            "Photos: train=6000\n",
            "Vocabulary Size: 7614\n",
            "Description Length: 34\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9qiMvOTGKxO"
      },
      "source": [
        "#### Testing Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_EicIvhwB9M",
        "outputId": "3b108d79-06d1-475a-aea1-4f98d4b12ca0"
      },
      "source": [
        " \r\n",
        "# dev dataset\r\n",
        " \r\n",
        "Test_img_IDs = '/content/Flicker8k_Dataset/Flickr_Data/Flickr_Data/Flickr_TextData/Flickr_8k.devImages.txt'\r\n",
        "\r\n",
        "\r\n",
        "# creating a set of Tresting image IDs\r\n",
        "test = load_set(Test_img_IDs)\r\n",
        "print('Dataset: %d' % len(test))\r\n",
        "\r\n",
        "\r\n",
        "# Create test descriptions\r\n",
        "test_descriptions = load_clean_descriptions( descriptions_path , test)\r\n",
        "print('Descriptions: test=%d' % len(test_descriptions))\r\n",
        "\r\n",
        "\r\n",
        "# create test features\r\n",
        "test_features = load_img_features( featurs_path , test)\r\n",
        "print('Photos: test=%d' % len(test_features))\r\n",
        "\r\n",
        "\r\n",
        "# prepare sequences\r\n",
        "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features)\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset: 1000\n",
            "Descriptions: test=1000\n",
            "Photos: test=1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouZblu4IHm-Z"
      },
      "source": [
        "### B. Save treaning and testing data as npz( compressed numpy files )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwRpr5nmGuBE",
        "outputId": "04b19d6c-fe17-4b34-f146-a71f0e634e0b"
      },
      "source": [
        "type(X1train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s07rgMJyH1V6",
        "outputId": "905f6077-e92a-4f39-e96f-2710407cc285"
      },
      "source": [
        "X1train.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "306455"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HmP8wzXH3U_",
        "outputId": "a352f3e3-960a-4ccb-8af9-dc4095aa336e"
      },
      "source": [
        "X1train.shape[0] == X2train.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1Sx6Jl-II5m",
        "outputId": "85a0069d-e4ad-43c7-b1e7-b5a417e176db"
      },
      "source": [
        "ytrain.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "306455"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSV9F1y1H8jM",
        "outputId": "3ab33625-330d-46ed-aa78-93dd7b283e5a"
      },
      "source": [
        "X1train.shape[0] == X2train.shape[0] == ytrain.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pT6PdrptIEQC",
        "outputId": "e78e9d29-6802-4a58-aa8f-c3873c9a0f7d"
      },
      "source": [
        "X1test.shape[0] == X2test.shape[0] == ytest.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEMQskv3Ii4O"
      },
      "source": [
        "base_dir = \"/content/gdrive/MyDrive/Imagecap/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqff24a_IRXU"
      },
      "source": [
        "from numpy import savez_compressed \r\n",
        "\r\n",
        "savez_compressed(base_dir + 'X1train.npz', X1train)\r\n",
        "savez_compressed(base_dir + 'X2train.npz', X2train)\r\n",
        "savez_compressed(base_dir + 'ytrain.npz', ytrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNtPbNDlIXSB"
      },
      "source": [
        "savez_compressed(base_dir + 'X1test.npz', X1test)\r\n",
        "savez_compressed(base_dir + 'X2test.npz', X2test)\r\n",
        "savez_compressed(base_dir + 'ytest.npz', ytest)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}